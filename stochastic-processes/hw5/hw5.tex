\documentclass[oneside,reqno]{amsart}
\setlength{\textwidth}{\paperwidth}\addtolength{\textwidth}{-2in}\calclayout
\usepackage{amsmath,amsthm}
\usepackage{dsfont} 
\usepackage{tikz}
\usepackage{enumitem}

\DeclareMathOperator{\var}{\mathrm{var}}
\DeclareMathOperator{\cov}{\mathrm{cov}}
\newcommand{\eps}{\varepsilon}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Pois}{\mathrm{Poisson}}
\newcommand{\Exp}{\mathrm{Exponential}}
\newcommand{\Bin}{\mathrm{Binomial}}
\newcommand{\Ber}{\mathrm{Bernoulli}}
\newcommand{\Geom}{\mathrm{Geometric}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\R}{\mathds{R}}
\newcommand{\N}{\mathds{N}}

\theoremstyle{definition}
\newtheorem{prob}{Problem}
\renewcommand*{\proofname}{Solution}
\setlist[enumerate]{label={(\roman*)}}

\title{STAT 433: Homework 5}
\author{Daniel Pfeffer}
%------------------------------------------------------------------------------
\begin{document}
\maketitle

\begin{prob}
Let $X$ and $Y$ be two independent Poisson random variables with $X \sim \Pois(\lambda)$ and $X \sim \Pois(\mu)$. Use probability generating functions to find the distribution of $X+Y$. 
\end{prob}

\begin{proof}
The PGF of $X$ is
\[
	G_X(s) = E s^X = \sum_{k=0}^\infty s^k e^{-\lambda} \frac{\lambda^k}{k!} = e^{-\lambda} \sum_{k=0}^\infty \frac{(s\lambda^k)}{k!} = e^{-\lambda} e^{s\lambda} = e^{\lambda(s-1)},
\]
and by the same logic, the PGF of $Y$ is $G_Y(s) =  e^{\mu(s-1)}$. By independence of $X$ and $Y$,
\[
	G_{X+Y}(s) = G_X(s)G_Y(s) = e^{\lambda(s-1)}e^{\mu(s-1)} = e^{(\lambda + \mu)(s-1)}.
\]
which, by uniqueness of the PGFs, $X+Y\sim \Pois(\lambda + \mu)$.
\end{proof}

\begin{prob}
Let $X_1, X_2, \dotsc$ be a sequence of i.i.d.\ Bernoulli random variables with parameter $p$. Let $N$ be a Poisson random variables with $N \sim \Pois(\lambda)$, which is independent of the $X_i$'s. 
\end{prob}

\begin{enumerate}
\item
Find the probability generating function of $Z=\sum_{i=1}^N X_i$.
\begin{proof}
By the law of total expectation, we have
\[
	G_Z(s) = E s^Z = E E(s^Z \mid N)
\]
For $N=n$, 
\begin{align*}
	E(s^Z \mid N= n) &= E(s^{\sum_{i=1}^N X_i} \mid N = n) 
	= E(s^{\sum_{i=1}^n X_i} \mid N = n)  \\
	&= E s^{\sum_{i=1}^n X_i}  
	= \prod_{i=1}^n E s^{X_i}
	= \prod_{i=1}^n (ps + (1-p)s) = (1 + p(s-1))^n,
\end{align*}
and so $E(s^Z \mid N)= (1 + p(s-1))^N$. Then, using the result from Problem 1, 
\[
	G_Z(s) = E (1+p(s-1))^N  = G_N((1+p(s-1))^N ) = e^{\lambda(1+p(s-1) - 1)} = e^{\lambda p(s-1)}.
\]
\end{proof}

\item
Use (1) to identify the probability distribution of $Z$.  
\begin{proof}
By the uniqueness property of PGFs, $Z \sim \Pois(\lambda p)$.
\end{proof}
\end{enumerate}


\begin{prob}
Consider a branching process with off-spring distribution 
\[
	\mathbf p = \left(t^2, 2t(1-t), (1-t)^2\right), \qquad 0 < t < 1,
\]	
i.e., it is binomial with parameters $2$ and $1-t$. Find the extinction probability. 
\end{prob}

\begin{proof}
Since the offspring distribution is $\Bin(2,1-t)$, we know its PGF is given by
\[
	G(s)  = (1 - (1-t) + (1-t)s)^2 = (t+s - ts)^2
\]
The mean is $\mu = G'(1) = 2- 2t$. When $t  \geq 1/2$, the mean $\mu \leq 1$, which corresponds to the subcritical and critical cases, the population goes extinct with probability 1.
\par
For $t < 1/2$, the mean is $\mu > 1$ and the process is supercritical, in which case the extinction probability is determined by the smallest positive root of the fixed point equation
\[
	G(s)=s \iff (1-t)^2 s^2 + (2 t - 2t^2 - 1)s + t^2 = 0,
\] 
which has solution 
\[
	s = \frac{-(2t - 2t^2 - 1) \pm \sqrt{(2t - 2t^2 - 1)^2 - 4t^2(1- t)^2}}{2(1- t)^2}
	=\frac{t^2}{(1-t)^2}.
\]
So, the extinction probability in the subcritical case is $t^2(1-t)^2$ for $t > 1/2$.
\end{proof}

\begin{prob}
\end{prob}

\begin{enumerate}
\item
A discrete random variable $X$ taking values in $\{0, 1, 2, \dotsc \}$ is said to be memoryless, if for any $m, n \geq 0$
\[	
	P(X > m + n \mid X > m) = P(X > n).
\]
Prove that $X$ is memoryless iff it is a geometric random variable.
\begin{proof}
Let $X$ be discrete nonnegative integer valued random variable satisfying the memoryless property. Then, by the definition of conditional probability,
\[
	P(X > m + n \mid X \geq m) = \frac{P(X>m + n)}{P(X \geq m)}
\]
together with the memoryless property implies that we have the functional equation
\[
	P(X > m + n) = P(X \geq m)P(X > n)
\] 
holds for any nonnegative integers $m$ and $n$. Now the functional equation remains to be solved. If we let $m=1$ and let $1-p = P(X \geq 1)$ and observe that 
\[
	P(X = 1+n \mid X > 1) =  \frac{P(X  =  n + 1, X > 1)}{P(X > 1)}  = \frac{P(X =n+1)}{1-p}  = P(X = n).
\]
we deduce the probabilities 
\begin{align*}
	P(X=2) &= (1-p) P(X=1) \\
	P(X=3) &= (1-p)P(X=2) = (1-p)^2 P(X=1) \\
	& \vdots \\ 
	P(X=k) &= (1-p)^k P(X=1),
\end{align*}	
and so on, for $k=0,1,2,\dotsc$, which is the PMF of a geometric random variable, and hence $X \sim \Geom(p)$. 
\par
Now let $X \sim \Geom(p)$ for $0 \leq p \leq 1$, which has tail probabilities given by
\[
	P(X > n) = \sum_{k=n}^\infty P(X = k) = \sum_{k=n}^\infty (1 - p)^{k-1} p.
\]
Letting $i=k-n$ gives
\[
	P(X > n) = \sum_{i=0}^\infty (1 - p)^{i+n-1} p = p(1-p)^{n-1} \sum_{i=0}^\infty (1 - p)^i = (1-p)^{n-1}.
\]
Now 
\[
	P(X > m + n \mid X > m)= \frac{P(X>m + n)}{P(X > m)} = \frac{(1-p)^{m+n-1}}{(1-p)^{m}} = (1-p)^{n-1}
\]
and hence $X$ satisfies the memoryless property. 
\end{proof}

\item
A continuous random variable $X$ taking values in $[0, \infty)$ is said to be memoryless, if for any $t,s\geq 0$ 
\[
	P(X > t + s \mid X > s) = P(X > t).
\]
Prove that $X$ is memoryless if it is an exponential random variable. 
\begin{proof}
Let $X \sim \Exp(\lambda)$. Then, by the definition of conditional probabilities (using the same logic as in part (1)) together with the fact that $P(X>t) = e^{-\lambda t}$, we have
\[
	P(X> t+s \mid X > s) =  \frac{P(X > t+s)}{P(X > s)} = \frac{e^{-\lambda(t+s)}}{e^{-\lambda s}} = e^{-\lambda t}.
\]
\end{proof}

\item
Suppose a continuous random variable $X$ taking values in $[0, \infty)$ is memoryless. Define the function $f(t) = P(X > t)$. Then $f(0)=1$, and by the memoryless property we have 
\[
	f(s+t) = f(s)f(t) \implies \frac{f(s+t) - f(t)}{s} = \frac{f(s) - f(0)}{s} f(t).
\] 
Show that by taking $s \to 0$, we obtain a differential equation 
\[
	f'(t) = f'(0)f(t).
\]
Solve the equation to show that form $e^{-\lambda t}$, which means that $X$ is an exponential random variable. 
\begin{proof}
Let $X$ be continuous nonnegative integer valued random variable satisfying the memoryless property. Letting $s \to 0$ gives
\[
	\lim_{s \to 0} \frac{f(s + t) - f(t)}{s} = f'(t),
\]
and
\[
	\lim_{s \to 0} \frac{f(s) - f(0)}{s}  = f'(0).
\]
This leads to an ordinary differential equation 
\[
	f'(t) = f'(0) f(t). 
\]
Since this ordinary differential equation is separable, we may write 
\[
	\frac{f'(t)}{f(t)} = f'(0),
\]
which implies 
\[
	\frac{d}{dt}\log f(t) = f'(0) 
\]
or 
\[
	\log f(t) = f'(0)t + c. 
\]
If we impose the initial condition that $f(0)=1$, then $c=0$ and 
\[
	f(t) = e^{f'(0) t}.
\]
Finally, setting $\lambda = - f'(0)$ gives 
\[
	f(t) = e^{-\lambda t},
\]
and therefore, $X$ is an exponential random variable.
\end{proof}
\end{enumerate}







\end{document}

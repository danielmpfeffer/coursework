\documentclass[oneside,reqno]{amsart}
\setlength{\textwidth}{\paperwidth}\addtolength{\textwidth}{-2in}\calclayout
\usepackage{amsmath,amsthm}
\usepackage{dsfont} 
\usepackage{enumitem}

\DeclareMathOperator{\E}{\mathrm{E}}
\DeclareMathOperator{\var}{\mathrm{var}}
\DeclareMathOperator{\cov}{\mathrm{cov}}
\newcommand{\eps}{\varepsilon}
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Ucal}{\mathcal{U}}
\newcommand{\Z}{\mathds{Z}}
\newcommand{\R}{\mathds{R}P}
\newcommand{\N}{\mathds{N}}

\theoremstyle{definition}
\newtheorem{prob}{Problem}
\renewcommand*{\proofname}{Solution}
\setlist[enumerate]{label={(\roman*)}}

\title{STAT 433: Homework 1}
\author{Daniel Pfeffer}
%------------------------------------------------------------------------------
\begin{document}
\maketitle


\begin{prob}
A poker hand consists of five cards drawn from a standard 52-card deck. Find the expected number of aces in a poker hand given that the first card drawn is an ace.
\end{prob}


\begin{proof}
Let $X$ be the number of aces in a poker hand, let $A_i$, for $i=1,2,\dotsc,5$, be the event that the $i$th card drawn is an ace, and let $1_{A_i}$ be an indicator function that equals one if the $A_i$ occurs and zero otherwise. Then we may write
\[
	X = 1_{A_1} + 1_{A_2} + \cdots + 1_{A_5}.
\]
Since we are conditioning on the event that the first card drawn is an ace, $1_{A_1} = 1$. It follows that the probability that one of the next four cars is an ace is $3/51 = 1/17$. By linearity of expectations, we obtain
\[
	\E X = \E 1_{A_1}  + \frac{1}{17} (\E 1_{A_1} + \E  1_{A_2} + \cdots + \E 1_{A_5})   
	= 1 + \frac{4}{17} 
	= \frac{21}{17}.
\]
\end{proof}


\begin{prob}
An urn has $n$ balls. Balls are drawn one at a time and then put back in the urn. Let $X$ denote the number of drawings required until some ball is drawn more than once. Find the probability distribution of $X$.
\end{prob}

\begin{proof}
Since it is impossible for a ball to be drawn twice if only one ball has been drawn, $P(X=1) = 0$. Note also that $P(X=2) = 1/n$ because there is only one ball (out of $n$) that can be drawn again. To deduce the probability that $X=3$, notice that the same ball is not drawn on the first trial with probability $1-P(X=1) = 1$ and on the second draw, the same ball is not drawn with probability $1-P(X=2) = 1-1/n$. Then, for the event $\{X=3\}$ to occur, the repeat draw must occur on the third trial, which happens with probability $2/n$ since there are 2 balls in the urn containing $n$ balls which can be drawn again:
\[
	P(X=3) =\left(1 - \frac{1}{n}\right)\left( \frac{2}{n}\right).
\]
Continuing with this reasoning, we obtain a probability distribution, for $1 \leq x \leq n+1$, given by
\[
	P(X=x) =\left(1-\frac{1}{n}\right)\left(1-\frac{2}{n}\right) \cdots\left(1-\frac{x-2}{n}\right) \left(\frac{x-1}{n}\right)
\]
\end{proof}


\begin{prob}
A man with $n$ keys wants to open his door. He tries the keys in a random manner. Let $X$ be the number of trials required to open the door. 
\end{prob}


\begin{enumerate}
\item
Find $\E(X)$ and $\var(X)$  if unsuccessful keys are eliminated from further selection.
\begin{proof}
Note that, since the number of keys $n$ decreases by 1 each trial, we have
\begin{align*}
	P(X=1) &= \frac{1}{n} \\
	P(X=2) &= \left(1-\frac{1}{n}\right) \frac{1}{n-1} \\
	&\vdots \\
	P(X=k) &= \left(1-\frac{1}{n}\right)\left(1-\frac{1}{n-1}\right) \cdots \left(1-\frac{1}{n-(k-2)}\right) \frac{1}{n-(k-1)},
\end{align*}
Then, the expectation of $X$ is
\begin{align*}
	\E (X) &= \frac{1}{n} + \frac{2}{n} + \cdots + \frac{n}{n} = \frac{n(n+1)}{2n} = \frac{n+1}{2}.
\end{align*}
To find the variance of $X$, compute the second moment:
\[
	\E (X^2) = \frac{1}{n} + \frac{4}{n} + \cdots + \frac{n^2}{n} = \frac{n(n+1)(2n+1)}{6n}=\frac{(n+1)(2n+1)}{6}.
\]
Using the fact that $\var(X) = \E (X^2) - (\E X)^2$ gives 
\[
	\var(X) = \frac{(n+1)(2n+1)}{6} - \left(  \frac{n+1}{2}  \right)^2 = \frac{n^2-1}{12}.
\]
\end{proof}
\item
Find $\E(X)$ and $\var(X)$ if unsuccessful keys are not eliminated from further selection.
\begin{proof}
If unsuccessful keys not are eliminated from further selection, then $X \sim \text{Geometric}(1/n)$, and hence 
\begin{align*}
	\E (X) &= \frac{1}{1/n} = n \\
	\var(X) &= \frac{1-1/n}{1/n^2} = n (n-1).
\end{align*}
\end{proof}

\end{enumerate}


\begin{prob}
Let $X_1,X_2,\dotsc, X_n$ be i.i.d. random variables with $X_i \sim \Ucal([0,t])$, and let $Y_n = \min\{ X_1,\dotsc, X_n\}$.
\end{prob}


\begin{enumerate}
\item
Find $P(Y_n > x)$ for $x \in [0,t]$.
\begin{proof}
Since $Y_n$ is greater than the minimum of $X_1, X_2,\dotsc, X_n$, we know $Y_n$ must also be greater than every $X_1, X_2,\dotsc, X_n$. So,
\begin{align*}
	P(Y_n > x) &= P(X_1 > x, X_2 > x, \dotsc, X_n > x) \\
					  &= \prod_{i=1}^n \left( 1- \frac{x}{t} \right) \\
					  &= \left( 1- \frac{x}{t} \right)^n
\end{align*}
\end{proof}
\item
Let $t$ be a function of $n$ such that $\lim_{n \to \infty} n/t= \lambda$. Show that 
\[
	\lim_{n \to \infty} P(Y_n > x) = e^{-\lambda x}.
\]
\begin{proof}
Evaluating the limit gives
\begin{align*} 
	\lim_{n \to \infty} P(Y_n > x) &= \lim_{n \to \infty} e^{n \log( 1- x/t)^n }\\
		&= \lim_{n \to \infty} e^{n(-x/t + o(1/t))} \\
		&= \lim_{n \to \infty} e^{n(-x\lambda / n + o(1/n))} \\
		&= e^{-\lambda x},
\end{align*}
since by definition $t$ satisfies $\lim_{n \to \infty} n/t = \lambda$.
\end{proof}
\end{enumerate}


\end{document}
